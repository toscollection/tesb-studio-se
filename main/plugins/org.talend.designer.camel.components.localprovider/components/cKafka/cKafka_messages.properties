LONG_NAME=Generic component for Kafka
HELP=org.talend.esb.help.cKafka
BROKER_LIST.NAME=Broker List
BROKER_LIST.DESCRIPTION=URL of the Kafka brokers to use. The format is host1:port1,host2:port2, and the list can be a subset of brokers or a VIP pointing to a subset of brokers.\n This option is known as bootstrap.servers in the Kafka documentation
CLIENT_ID.NAME=Client Id
CLIENT_ID.DESCRIPTION=The client id is a user-specified string sent in each request to help trace calls. \nIt should logically identify the application making the request.
TOPIC.NAME=Topic
TOPIC.DESCRIPTION=Name of topic
GROUP_ID.NAME=Group Id
GROUP_ID.DESCRIPTION=A string that uniquely identifies the group of consumer processes to which this consumer belongs. \nBy setting the same group id multiple processes indicate that they are all part of the same consumer group.
PARTITIONER.NAME=Partitioner
PARTITIONER.DESCRIPTION=The partitioner class for partitioning messages amongst sub-topics. \nThe default partitioner is based on the hash of the key.
URI_OPTIONS.NAME=Kafka Properties
URI_OPTIONS.ITEM.NAME=Name
URI_OPTIONS.ITEM.VALUE=Value
COMPRESSION_CODEC.NAME=Compression Codec
COMPRESSION_CODEC.DESCRIPTION=This parameter allows you to specify the compression codec for all data generated by this producer. \nValid values are none, gzip, snappy, lz4 and zstd.
COMPRESSION_CODEC.ITEM.NONE=NONE
COMPRESSION_CODEC.ITEM.GZIP=GZIP
COMPRESSION_CODEC.ITEM.SNAPPY=SNAPPY
COMPRESSION_CODEC.ITEM.LZ4=LZ4
REQUEST_REQUIRED_ACKS.NAME=Request Required Acks
REQUEST_REQUIRED_ACKS.DESCRIPTION=The number of acknowledgments the producer requires the leader to have received before considering a request complete. \nThis controls the durability of records that are sent. \nThe following settings are allowed: acks=0 If set to zero then the producer will not wait for any acknowledgment from the server at all. \nThe record will be immediately added to the socket buffer and considered sent. \nNo guarantee can be made that the server has received the record in this case, and the retries configuration will not take effect (as the client won’t generally know of any failures).\n The offset given back for each record will always be set to -1. \nacks=1 This will mean the leader will write the record to its local log but will respond without awaiting full acknowledgement from all followers. \nIn this case should the leader fail immediately after acknowledging the record but before the followers have replicated it then the record will be lost. \nacks=all This means the leader will wait for the full set of in-sync replicas to acknowledge the record. \nThis guarantees that the record will not be lost as long as at least one in-sync replica remains alive. \nThis is the strongest available guarantee. This is equivalent to the acks=-1 setting. \nNote that enabling idempotence requires this config value to be 'all'. If conflicting configurations are set and idempotence is not explicitly enabled, idempotence is disabled.
REQUEST_REQUIRED_ACKS.ITEM.MINUS_ONE=-1
REQUEST_REQUIRED_ACKS.ITEM.ZERO=0
REQUEST_REQUIRED_ACKS.ITEM.ONE=1
REQUEST_REQUIRED_ACKS.ITEM.ALL=ALL
RETRY_BACKOFF_MS.NAME=Retry Backoff (ms)
RETRY_BACKOFF_MS.DESCRIPTION=Before each retry, the producer refreshes the metadata of relevant topics to see if a new leader has been elected. \nSince leader election takes a bit of time, this property specifies the amount of time that the producer waits before refreshing the metadata.
SEND_BUFFER_BYTES.NAME=Send Buffer (bytes)
SEND_BUFFER_BYTES.DESCRIPTION=Socket write buffer size.
REQUEST_TIMEOUT_MS.NAME=Request Timeout (ms)
REQUEST_TIMEOUT_MS.DESCRIPTION=The amount of time the broker will wait trying to meet the request.required.acks requirement before sending back an error to the client.
SERIALIZER_CLASS.NAME=Serializer Class
SERIALIZER_CLASS.DESCRIPTION=The serializer class for messages.
KEY_SERIALIZER_CLASS.NAME=Key Serializer Class
KEY_SERIALIZER_CLASS.DESCRIPTION=The serializer class for keys (defaults to the same as for messages if nothing is given).
AUTO_COMMIT_ENABLE.NAME=Auto Commit Enable
AUTO_COMMIT_ENABLE.DESCRIPTION=If true, periodically commit to ZooKeeper the offset of messages already fetched by the consumer. \nThis committed offset will be used when the process fails as the position from which the new consumer will begin.
AUTO_COMMIT_INTERVAL_MS.NAME=Auto Commit Interval (ms)
AUTO_COMMIT_INTERVAL_MS.DESCRIPTION=The frequency in ms that the consumer offsets are committed to zookeeper.
FETCH_MIN_BYTES.NAME=Fetch Min (bytes)
FETCH_MIN_BYTES.DESCRIPTION=The minimum amount of data the server should return for a fetch request. \nIf insufficient data is available the request will wait for that much data to accumulate before answering the request.
FETCH_WAIT_MAX_MS.NAME=Fetch Wait Max (ms)
FETCH_WAIT_MAX_MS.DESCRIPTION=The maximum amount of time the server will block before answering the fetch request if there isn’t sufficient data to immediately satisfy fetch.min.bytes.
AUTO_OFFSET_RESET.NAME=Auto Offset Reset
AUTO_OFFSET_RESET.DESCRIPTION=What to do when there is no initial offset in ZooKeeper or if an offset is out of range: \nearliest : automatically reset the offset to the earliest offset \nlatest : automatically reset the offset to the latest offset \nfail: throw exception to the consumer.
AUTO_OFFSET_RESET.ITEM.LATEST=LATEST
AUTO_OFFSET_RESET.ITEM.EARLIEST=EARLIEST
AUTO_OFFSET_RESET.ITEM.NONE=NONE
SSL.NAME=SSL
SSL_KEY_PASSWORD.NAME=SSL Key Password
SSL_KEY_PASSWORD.DESCRIPTION=The password of the private key in the key store file or the PEM key specified in sslKeystoreKey. \nThis is required for clients only if two-way authentication is configured.
SSL_KEYSTORE_LOCATION.NAME=SSL Keystore Location
SSL_KEYSTORE_LOCATION.DESCRIPTION=The location of the key store file. This is optional for client and can be used for two-way authentication for client.
SSL_KEYSTORE_PASSWORD.NAME=SSL Keystore Password
SSL_KEYSTORE_PASSWORD.DESCRIPTION=The store password for the key store file. This is optional for client and only needed if sslKeystoreLocation' is configured. \nKey store password is not supported for PEM format.
SSL_TRUSTSTORE_LOCATION.NAME=SSL Truststore Location
SSL_TRUSTSTORE_LOCATION.DESCRIPTION=The location of the trust store file.
SSL_TRUSTSTORE_PASSWORD.NAME=SSL Truststore Password
SSL_TRUSTSTORE_PASSWORD.DESCRIPTION=The password for the trust store file. If a password is not set, trust store file configured will still be used, but integrity checking is disabled. \nTrust store password is not supported for PEM format.
SSL_CIPHER_SUITS.NAME=SSL Cipher Suites
SSL_CIPHER_SUITS.DESCRIPTION=A list of cipher suites. This is a named combination of authentication, encryption, \nMAC and key exchange algorithm used to negotiate the security settings for a network connection using TLS or SSL network protocol. \nBy default all the available cipher suites are supported.
SSL_ENDPOINT_ALGORITHM.NAME=SSL Endpoint Algorithm
SSL_ENDPOINT_ALGORITHM.DESCRIPTION=The endpoint identification algorithm to validate server hostname using server certificate. \nUse none or false to disable server hostname verification.
SASL.NAME=SASL
SASL_KERBEROS_SERVICE_NAME.NAME=Kerberos Service Name
SASL_KERBEROS_SERVICE_NAME.DESCRIPTION=The Kerberos principal name that Kafka runs as. This can be defined either in Kafka’s JAAS config or in Kafka’s config.
SECURITY_PROTOCOL.NAME=Security Protocol
SECURITY_PROTOCOL.DESCRIPTION=Protocol used to communicate with brokers. SASL_PLAINTEXT, PLAINTEXT, SASL_SSL and SSL are supported.
SECURITY_PROTOCOL.ITEM.PLAINTEXT=Plaintext
SECURITY_PROTOCOL.ITEM.SSL=SSL
SECURITY_PROTOCOL.ITEM.SASL_PLAINTEXT=SASL over Plaintext
SECURITY_PROTOCOL.ITEM.SASL_SSL=SASL over SSL
HEARTBEAT_INTERVAL_MS.NAME=Heartbeat Interval (ms)
HEARTBEAT_INTERVAL_MS.DESCRIPTION=The expected time between heartbeats to the consumer coordinator when using Kafka’s group management facilities. \nHeartbeats are used to ensure that the consumer’s session stays active and to facilitate rebalancing when new consumers join or leave the group. \nThe value must be set lower than session.timeout.ms, but typically should be set no higher than 1/3 of that value. \nIt can be adjusted even lower to control the expected time for normal rebalances.
MAX_PARTITION_FETCH_BYTES.NAME=Maximum Partition Fetch (bytes)
MAX_PARTITION_FETCH_BYTES.DESCRIPTION=The maximum amount of data per-partition the server will return. The maximum total memory used for a request will be #partitions max.partition.fetch.bytes. \nThis size must be at least as large as the maximum message size the server allows or else it is possible for the producer to send messages larger than the consumer can fetch. \n If that happens, the consumer can get stuck trying to fetch a large message on a certain partition.
SESSION_TIMEOUT_MS.NAME=Session Timeout (ms)
SESSION_TIMEOUT_MS.DESCRIPTION=The timeout used to detect failures when using Kafka’s group management facilities.=Session Timeout (ms)
PARTITION_ASSIGNOR.NAME=Partition Assignor
PARTITION_ASSIGNOR.DESCRIPTION=The class name of the partition assignment strategy that the client will use to distribute partition ownership amongst consumer instances when group management is used.
CONSUMER_REQUEST_TIMEOUT_MS.NAME=Request Timeout (ms)
CONSUMER_REQUEST_TIMEOUT_MS.DESCRIPTION=The configuration controls the maximum amount of time the client will wait for the response of a request. \nIf the response is not received before the timeout elapses the client will resend the request if necessary or fail the request if retries are exhausted.
BUFFER_MEMORY_SIZE.NAME=Buffer Memory Size
BUFFER_MEMORY_SIZE.DESCRIPTION=The total bytes of memory the producer can use to buffer records waiting to be sent to the server. \nIf records are sent faster than they can be delivered to the server the producer will either block or throw an exception based on the preference specified by block.on.buffer.full. \nThis setting should correspond roughly to the total memory the producer will use, but is not a hard bound since not all memory the producer uses is used for buffering. \nSome additional memory will be used for compression (if compression is enabled) as well as for maintaining in-flight requests.
RETRIES.NAME=Retries
RETRIES.DESCRIPTION=Setting a value greater than zero will cause the client to resend any record whose send fails with a potentially transient error. \nNote that this retry is no different than if the client resent the record upon receiving the error. \nProduce requests will be failed before the number of retries has been exhausted if the timeout configured by delivery.timeout.ms expires first before successful acknowledgement.\n Users should generally prefer to leave this config unset and instead use delivery.timeout.ms to control retry behavior. \nEnabling idempotence requires this config value to be greater than 0. If conflicting configurations are set and idempotence is not explicitly enabled, idempotence is disabled. \nAllowing retries while setting enable.idempotence to false and max.in.flight.requests.per.connection to 1 will potentially change the ordering of records because if two batches are sent to a single partition, \nand the first fails and is retried but the second succeeds, then the records in the second batch may appear first.
PRODUCER_BATCH_SIZE.NAME=Batch Size
PRODUCER_BATCH_SIZE.DESCRIPTION=The producer will attempt to batch records together into fewer requests whenever multiple records are being sent to the same partition. \nThis helps performance on both the client and the server. This configuration controls the default batch size in bytes. \nNo attempt will be made to batch records larger than this size. Requests sent to brokers will contain multiple batches, one for each partition with data available to be sent. \nA small batch size will make batching less common and may reduce throughput (a batch size of zero will disable batching entirely). \nA very large batch size may use memory a bit more wastefully as we will always allocate a buffer of the specified batch size in anticipation of additional records.
CONNECTION_MAX_IDLE_MS.NAME=Connection Idle Max (ms)
CONNECTION_MAX_IDLE_MS.DESCRIPTION=Close idle connections after the number of milliseconds specified by this config.
LINGER_MS.NAME=Linger (ms)
LINGER_MS.DESCRIPTION=The producer groups together any records that arrive in between request transmissions into a single batched request. \nNormally this occurs only under load when records arrive faster than they can be sent out. However in some circumstances the client may want to reduce the number of requests even under moderate load. \nThis setting accomplishes this by adding a small amount of artificial delay that is, rather than immediately sending out a record the producer will wait for up to the given delay to allow other records to be sent, so that the sends can be batched together. \nThis can be thought of as analogous to Nagle’s algorithm in TCP. This setting gives the upper bound on the delay for batching: \nonce we get batch.size worth of records for a partition, the records will be sent immediately regardless of this setting, however if we have fewer than this many bytes accumulated for this partition， will 'linger' for the specified time， waiting for more records to show up. \nThis setting defaults to 0 (i.e. no delay). Setting linger.ms=5, for example, would have the effect of reducing the number of requests sent, but would add up to 5ms of latency to records sent in the absence of load.
MAX_BLOCK_MS.NAME=Max Block (ms)
MAX_BLOCK_MS.DESCRIPTION=The configuration controls how long the KafkaProducer’s send(), partitionsFor(), initTransactions(), sendOffsetsToTransaction(), commitTransaction() and abortTransaction() methods will block. \nFor send() this timeout bounds the total time waiting for both metadata fetch and buffer allocation (blocking in the user-supplied serializers or partitioner is not counted against this timeout). \nFor partitionsFor() this timeout bounds the time spent waiting for metadata if it is unavailable. \nThe transaction-related methods always block, but may timeout if the transaction coordinator could not be discovered or did not respond within the timeout.
MAX_REQUEST_SIZE.NAME=Max Request Size
MAX_REQUEST_SIZE.DESCRIPTION=The maximum size of a request. This is also effectively a cap on the maximum record size. \nNote that the server has its own cap on record size which may be different from this. \nThis setting will limit the number of record batches the producer will send in a single request to avoid sending huge requests.
RECEIVE_BUFFER_BYTES.NAME=Receive Buffer (bytes)
RECEIVE_BUFFER_BYTES.DESCRIPTION=The size of the TCP receive buffer (SO_RCVBUF) to use when reading data.
MAX_IN_FLIGHT_REQUEST.NAME=Max in Flight Request
MAX_IN_FLIGHT_REQUEST.DESCRIPTION=The maximum number of unacknowledged requests the client will send on a single connection before blocking. \nNote that if this setting is set to be greater than 1 and there are failed sends, there is a risk of message re-ordering due to retries (i.e., if retries are enabled).
METADATA_MAX_AGE_MS.NAME=Metadata Max Age (ms)
METADATA_MAX_AGE_MS.DESCRIPTION=The period of time in milliseconds after which there will be a force refresh of metadata， even if no any partition leadership changes to proactively discover any new brokers or partitions.
RECONNECT_BACKOFF_MS.NAME=Reconnect Backoff (ms)
RECONNECT_BACKOFF_MS.DESCRIPTION=The amount of time to wait before attempting to reconnect to a given host. This avoids repeatedly connecting to a host in a tight loop. \nThis backoff applies to all requests sent by the consumer to the broker.
URI_OPTIONS.DESCRIPTION=Kafka properties
USE_SCHEMA_REGISTRY.NAME=Use Schema Registry
SCHEMA_REGISTRY_URL.NAME=Schema Registry URL
SCHEMA_REGISTRY_URL.DESCRIPTION=URL of the Confluent Platform schema registry servers to use. The format is host1:port1,host2:port2. \nThis is known as schema.registry.url in the Confluent Platform documentation. \nThis option is only available in the Confluent Platform (not standard Apache Kafka).
